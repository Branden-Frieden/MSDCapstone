{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchmetrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[136], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39moptim\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclassification\u001b[39;00m \u001b[39mimport\u001b[39;00m MulticlassConfusionMatrix\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpathlib\u001b[39;00m \u001b[39mimport\u001b[39;00m Path\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchmetrics'"
     ]
    }
   ],
   "source": [
    "# Include Libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.classification import MulticlassConfusionMatrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn folders into Train and Test sets\n",
    "for dirpath, dirnames, filenames in os.walk(\"data/asl_dataset\"):\n",
    "  for filename in (filenames):\n",
    "\n",
    "    # generate random number between 1 and 10\n",
    "    rand_int = random.randint(1, 10)\n",
    "\n",
    "    # use rand_int to determine whether image goes to test or train\n",
    "\n",
    "    if rand_int >= 9:\n",
    "      split_path = Path(\"data/test\")\n",
    "    else:\n",
    "      split_path = Path(\"data/train\")\n",
    "\n",
    "    # split the full path so we can pull the class folder name later\n",
    "    sub_paths = dirpath.split(\"/\")\n",
    "\n",
    "    # generate souce path and destination path\n",
    "    src_path = Path(dirpath) / filename\n",
    "    dest_path = Path(split_path) / Path(sub_paths[len(sub_paths) - 1]) / filename\n",
    "\n",
    "    # Copy image to location, or create destination then copy\n",
    "    try:\n",
    "      shutil.copyfile(src_path, dest_path)\n",
    "    except IOError as io_err:\n",
    "      os.makedirs(os.path.dirname(dest_path))\n",
    "      shutil.copyfile(src_path, dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 36 directories and 0 images in 'data/asl_dataset'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/r'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/u'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/9'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/0'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/7'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/i'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/n'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/g'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/6'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/z'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/1'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/8'.\n",
      "There are 0 directories and 65 images in 'data/asl_dataset/t'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/s'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/a'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/f'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/o'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/h'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/m'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/j'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/c'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/d'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/v'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/q'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/4'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/x'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/3'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/e'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/b'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/k'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/l'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/2'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/y'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/5'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/p'.\n",
      "There are 0 directories and 70 images in 'data/asl_dataset/w'.\n",
      "\n",
      "\n",
      "There are 36 directories and 0 images in 'data/test'.\n",
      "There are 0 directories and 62 images in 'data/test/r'.\n",
      "There are 0 directories and 58 images in 'data/test/u'.\n",
      "There are 0 directories and 59 images in 'data/test/9'.\n",
      "There are 0 directories and 60 images in 'data/test/0'.\n",
      "There are 0 directories and 60 images in 'data/test/7'.\n",
      "There are 0 directories and 63 images in 'data/test/i'.\n",
      "There are 0 directories and 56 images in 'data/test/n'.\n",
      "There are 0 directories and 62 images in 'data/test/g'.\n",
      "There are 0 directories and 61 images in 'data/test/6'.\n",
      "There are 0 directories and 60 images in 'data/test/z'.\n",
      "There are 0 directories and 62 images in 'data/test/1'.\n",
      "There are 0 directories and 64 images in 'data/test/8'.\n",
      "There are 0 directories and 57 images in 'data/test/t'.\n",
      "There are 0 directories and 63 images in 'data/test/s'.\n",
      "There are 0 directories and 60 images in 'data/test/a'.\n",
      "There are 0 directories and 61 images in 'data/test/f'.\n",
      "There are 0 directories and 61 images in 'data/test/o'.\n",
      "There are 0 directories and 58 images in 'data/test/h'.\n",
      "There are 0 directories and 61 images in 'data/test/m'.\n",
      "There are 0 directories and 56 images in 'data/test/j'.\n",
      "There are 0 directories and 62 images in 'data/test/c'.\n",
      "There are 0 directories and 55 images in 'data/test/d'.\n",
      "There are 0 directories and 61 images in 'data/test/v'.\n",
      "There are 0 directories and 66 images in 'data/test/q'.\n",
      "There are 0 directories and 62 images in 'data/test/4'.\n",
      "There are 0 directories and 61 images in 'data/test/x'.\n",
      "There are 0 directories and 63 images in 'data/test/3'.\n",
      "There are 0 directories and 61 images in 'data/test/e'.\n",
      "There are 0 directories and 58 images in 'data/test/b'.\n",
      "There are 0 directories and 63 images in 'data/test/k'.\n",
      "There are 0 directories and 60 images in 'data/test/l'.\n",
      "There are 0 directories and 62 images in 'data/test/2'.\n",
      "There are 0 directories and 61 images in 'data/test/y'.\n",
      "There are 0 directories and 62 images in 'data/test/5'.\n",
      "There are 0 directories and 54 images in 'data/test/p'.\n",
      "There are 0 directories and 62 images in 'data/test/w'.\n",
      "\n",
      "\n",
      "There are 36 directories and 0 images in 'data/train'.\n",
      "There are 0 directories and 70 images in 'data/train/r'.\n",
      "There are 0 directories and 70 images in 'data/train/u'.\n",
      "There are 0 directories and 70 images in 'data/train/9'.\n",
      "There are 0 directories and 70 images in 'data/train/0'.\n",
      "There are 0 directories and 70 images in 'data/train/7'.\n",
      "There are 0 directories and 70 images in 'data/train/i'.\n",
      "There are 0 directories and 70 images in 'data/train/n'.\n",
      "There are 0 directories and 70 images in 'data/train/g'.\n",
      "There are 0 directories and 70 images in 'data/train/6'.\n",
      "There are 0 directories and 70 images in 'data/train/z'.\n",
      "There are 0 directories and 70 images in 'data/train/1'.\n",
      "There are 0 directories and 70 images in 'data/train/8'.\n",
      "There are 0 directories and 65 images in 'data/train/t'.\n",
      "There are 0 directories and 70 images in 'data/train/s'.\n",
      "There are 0 directories and 70 images in 'data/train/a'.\n",
      "There are 0 directories and 70 images in 'data/train/f'.\n",
      "There are 0 directories and 70 images in 'data/train/o'.\n",
      "There are 0 directories and 70 images in 'data/train/h'.\n",
      "There are 0 directories and 70 images in 'data/train/m'.\n",
      "There are 0 directories and 70 images in 'data/train/j'.\n",
      "There are 0 directories and 70 images in 'data/train/c'.\n",
      "There are 0 directories and 70 images in 'data/train/d'.\n",
      "There are 0 directories and 70 images in 'data/train/v'.\n",
      "There are 0 directories and 70 images in 'data/train/q'.\n",
      "There are 0 directories and 70 images in 'data/train/4'.\n",
      "There are 0 directories and 70 images in 'data/train/x'.\n",
      "There are 0 directories and 70 images in 'data/train/3'.\n",
      "There are 0 directories and 70 images in 'data/train/e'.\n",
      "There are 0 directories and 70 images in 'data/train/b'.\n",
      "There are 0 directories and 70 images in 'data/train/k'.\n",
      "There are 0 directories and 70 images in 'data/train/l'.\n",
      "There are 0 directories and 70 images in 'data/train/2'.\n",
      "There are 0 directories and 70 images in 'data/train/y'.\n",
      "There are 0 directories and 70 images in 'data/train/5'.\n",
      "There are 0 directories and 70 images in 'data/train/p'.\n",
      "There are 0 directories and 70 images in 'data/train/w'.\n"
     ]
    }
   ],
   "source": [
    "# check for proper test/train split (aproxamatly 20 %)\n",
    "for dirpath, dirnames, filenames in os.walk(Path(\"data/asl_dataset\")):\n",
    "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(Path(\"data/test\")):\n",
    "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(Path(\"data/train\")):\n",
    "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up transformer\n",
    "data_transform = transforms.Compose([\n",
    "    # Resize our images to 64 x 64\n",
    "    transforms.Resize(size=(64, 64)),\n",
    "    #Turn image into torch.Tensor\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset ImageFolder\n",
       "     Number of datapoints: 2515\n",
       "     Root location: data/train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=warn)\n",
       "                ToTensor()\n",
       "            ),\n",
       " Dataset ImageFolder\n",
       "     Number of datapoints: 2177\n",
       "     Root location: data/test\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=warn)\n",
       "                ToTensor()\n",
       "            ))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up the directories to the images\n",
    "train_dir = \"data/train\"\n",
    "test_dir = \"data/test\"\n",
    "\n",
    "# pull the images into datasets\n",
    "train_data = datasets.ImageFolder(root=train_dir,\n",
    "                                  transform=data_transform,  # a transform for the data\n",
    "                                  target_transform=None) # a transform for the label/target\n",
    "\n",
    "test_data = datasets.ImageFolder(root=test_dir,\n",
    "                                  transform=data_transform,  # a transform for the data\n",
    "                                  target_transform=None) # a transform for the label/target\n",
    "\n",
    "train_data, test_data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x15719ef50>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x157184950>)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up dataloader\n",
    "BATCH_SIZE = 8\n",
    "train_dataloader = DataLoader(dataset=train_data,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              num_workers=0,\n",
    "                              shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              num_workers=0,\n",
    "                              shuffle=False)\n",
    "\n",
    "train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, outputs):\n",
    "        super(Net, self).__init__()\n",
    "        c1out = 6\n",
    "        c2out = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, c1out, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.norm1 = nn.BatchNorm2d(c1out)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(c1out, c2out, 3)\n",
    "        self.norm2 = nn.BatchNorm2d(c2out)\n",
    "\n",
    "        self.pooledOutputSize = c2out * 14 * 14 \n",
    "        self.fc1 = nn.Linear( self.pooledOutputSize, 120 )\n",
    "        self.fc2 = nn.Linear( 120, outputs )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.norm1(self.pool(F.relu(self.conv1(x))))\n",
    "\n",
    "        x = self.norm2(self.pool(F.relu(self.conv2(x))))\n",
    "\n",
    "        x = x.view(-1, self.num_flat_features(x)) \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    #compute the output size after our convolution layers\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( model, epochs, dataloader ): # One epoch uses the entire training set (one batch at a time) - 60,000 images in this case\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "    optimizer = optim.Adam( model.parameters(), lr= 1e-4 ) \n",
    "    trainloader = dataloader\n",
    "\n",
    "    for epoch in range( epochs ): # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate( trainloader, 0 ):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "\n",
    "            inputs, labels = data\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs) #predict the output with some training data\n",
    "            loss = criterion(outputs, labels) #see how well we did\n",
    "\n",
    "            loss.backward() #see how to change the weights to do better\n",
    "            optimizer.step() #and actually change the weights\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 2.704\n",
      "[1,   200] loss: 1.242\n",
      "[1,   300] loss: 0.711\n",
      "[2,   100] loss: 0.428\n",
      "[2,   200] loss: 0.314\n",
      "[2,   300] loss: 0.285\n",
      "[3,   100] loss: 0.178\n",
      "[3,   200] loss: 0.177\n",
      "[3,   300] loss: 0.149\n",
      "[4,   100] loss: 0.110\n",
      "[4,   200] loss: 0.100\n",
      "[4,   300] loss: 0.084\n",
      "[5,   100] loss: 0.069\n",
      "[5,   200] loss: 0.058\n",
      "[5,   300] loss: 0.059\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net = Net(36)\n",
    "train(net, 5, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate( model, dataloader ):  \n",
    "\n",
    "    testloader = dataloader\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad(): # <- Since we are not training, the model does not need to calculate gradients\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = model( images )\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Just do a coarse evaluation... how many did we predict correcly?\n",
    "    print( 'Accuracy of the network on the 10000 test images: %f %%' % ( 100 * correct / total) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 99.862196 %\n"
     ]
    }
   ],
   "source": [
    "evaluate(net, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
